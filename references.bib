
@article{mishra_collaborative_2023,
	title = {A {Collaborative} {Computation} and {Offloading} for {Compute}-{Intensive} and {Latency}-{Sensitive} {Dependency}-{Aware} {Tasks} in {Dew}-{Enabled} {Vehicular} {Fog} {Computing}: {A} {Federated} {Deep} {Q}-{Learning} {Approach}},
	volume = {20},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {1932-4537, 2373-7379},
	shorttitle = {A {Collaborative} {Computation} and {Offloading} for {Compute}-{Intensive} and {Latency}-{Sensitive} {Dependency}-{Aware} {Tasks} in {Dew}-{Enabled} {Vehicular} {Fog} {Computing}},
	url = {https://ieeexplore.ieee.org/document/10143987/},
	doi = {10.1109/TNSM.2023.3282795},
	abstract = {The demand for vehicular networks is proliﬁcally emerging as it supports advancing in capabilities and qualities of vehicle services. However, this vehicular network cannot solely carry out latency-sensitive and compute-intensive tasks, as the slightest delay may cause any catastrophe. Therefore, fog computing can be a viable solution as an integration to address the aforementioned challenges. Moreover, it complements Cloud computing as it reduces the incurred latency and ingress trafﬁc by shifting the computing resources to the edge of the network. This work investigated task ofﬂoading methods in Vehicular Fog Computing (VFC) networks and proposes a Federated learning-supported Deep Q-Learning-based (FedDQL) technique for optimal ofﬂoading of tasks in a collaborative computing paradigm. The proposed ofﬂoading method in the VFC network performs computations, communications, ofﬂoading, and resource utilization considering the latency and energy consumption. The trade-offs between latency and computing and communication constraints were considered in this scenario. The FedDQL scheme was validated for dependent task sets to analyze the efﬁcacy of this method. Finally, the results of extensive simulations provide evidence that the proposed method outperforms others with an average improvement of 49\%, 34.3\%, 29.2\%, 16.2\% and 8.21\%, respectively.},
	language = {en},
	number = {4},
	urldate = {2024-12-01},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Mishra, Kaushik and Rajareddy, Goluguri N. V. and Ghugar, Umashankar and Chhabra, Gurpreet Singh and Gandomi, Amir H.},
	month = dec,
	year = {2023},
	pages = {4600--4614},
	file = {PDF:/home/arthur/Zotero/storage/9GY2VLV5/Mishra et al. - 2023 - A Collaborative Computation and Offloading for Compute-Intensive and Latency-Sensitive Dependency-Aw.pdf:application/pdf},
}

@article{benaboura_comprehensive_nodate,
	title = {A comprehensive survey of task offloading techniques in {IoT}-{Fog}-{Cloud} computing},
	abstract = {The proliferation of IoT devices has led to a surge in the data generated by these devices. This creates challenges in managing, processing, and analyzing this data in a timely and efficient manner. The IoT Fog-Cloud computing paradigm has emerged as a promising solution to address these challenges by enabling data processing and storage at different layers of the IoT architecture. In this paper, we present a comprehensive survey of the state-of-the-art task offloading techniques in IoT Fog Cloud computing. We will first introduce the IoT-Fog-Cloud architecture and its main functions. Then, we examine different types of task offloading that are included in the full or complete offloading, and the associated advantages and challenges. We examine various factors that influence the decision to offload tasks. Next, we examine the various task offloading strategies that have been proposed for IoT-Fog-Cloud computing. We also discuss the trade-off between these approaches in terms of different evaluation parameters such as energy consumption, latency, response time, and cost. Finally, we highlight the open research challenges and future directions for task offloading in IoT-Fog-Cloud computing.},
	language = {en},
	author = {Benaboura, Amina and Bechar, Rachid and Kadri, Walid},
	file = {PDF:/home/arthur/Zotero/storage/VSQEHSVK/Benaboura et al. - A comprehensive survey of task offloading techniques in IoT-Fog-Cloud computing.pdf:application/pdf},
}

@article{alsadie_advancements_2024,
	title = {Advancements in heuristic task scheduling for {IoT} applications in fog-cloud computing: challenges and prospects},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2376-5992},
	shorttitle = {Advancements in heuristic task scheduling for {IoT} applications in fog-cloud computing},
	url = {https://peerj.com/articles/cs-2128},
	doi = {10.7717/peerj-cs.2128},
	abstract = {Fog computing has emerged as a prospective paradigm to address the computational requirements of IoT applications, extending the capabilities of cloud computing to the network edge. Task scheduling is pivotal in enhancing energy efﬁciency, optimizing resource utilization and ensuring the timely execution of tasks within fog computing environments. This article presents a comprehensive review of the advancements in task scheduling methodologies for fog computing systems, covering priority-based, greedy heuristics, metaheuristics, learning-based, hybrid heuristics, and nature-inspired heuristic approaches. Through a systematic analysis of relevant literature, we highlight the strengths and limitations of each approach and identify key challenges facing fog computing task scheduling, including dynamic environments, heterogeneity, scalability, resource constraints, security concerns, and algorithm transparency. Furthermore, we propose future research directions to address these challenges, including the integration of machine learning techniques for real-time adaptation, leveraging federated learning for collaborative scheduling, developing resource-aware and energy-efﬁcient algorithms, incorporating securityaware techniques, and advancing explainable AI methodologies. By addressing these challenges and pursuing these research directions, we aim to facilitate the development of more robust, adaptable, and efﬁcient task-scheduling solutions for fog computing environments, ultimately fostering trust, security, and sustainability in fog computing systems and facilitating their widespread adoption across diverse applications and domains.},
	language = {en},
	urldate = {2024-12-01},
	journal = {PeerJ Computer Science},
	author = {Alsadie, Deafallah},
	month = jun,
	year = {2024},
	pages = {e2128},
	file = {PDF:/home/arthur/Zotero/storage/3AM5J77D/Alsadie - 2024 - Advancements in heuristic task scheduling for IoT applications in fog-cloud computing challenges an.pdf:application/pdf},
}

@article{jeyaraman_applications_2024,
	title = {Applications of {Fog} {Computing} in {Healthcare}},
	issn = {2168-8184},
	url = {https://www.cureus.com/articles/270625-applications-of-fog-computing-in-healthcare},
	doi = {10.7759/cureus.64263},
	abstract = {Fog computing is a decentralized computing infrastructure that processes data at or near its source, reducing latency and bandwidth usage. This technology is gaining traction in healthcare due to its potential to enhance real-time data processing and decision-making capabilities in critical medical scenarios.},
	language = {en},
	urldate = {2024-12-01},
	journal = {Cureus},
	author = {Jeyaraman, Naveen and Jeyaraman, Madhan and Yadav, Sankalp and Ramasubramanian, Swaminathan and Balaji, Sangeetha and Muthu, Sathish and Lekha P, Chithra and Patro, Bishnu P},
	month = jul,
	year = {2024},
	file = {PDF:/home/arthur/Zotero/storage/Y4LVBS8A/Jeyaraman et al. - 2024 - Applications of Fog Computing in Healthcare.pdf:application/pdf},
}

@misc{fahimullah_review_2022,
	title = {A {Review} of {Resource} {Management} in {Fog} {Computing}: {Machine} {Learning} {Perspective}},
	shorttitle = {A {Review} of {Resource} {Management} in {Fog} {Computing}},
	url = {http://arxiv.org/abs/2209.03066},
	doi = {10.48550/arXiv.2209.03066},
	abstract = {Fog computing becomes a promising technology to process user’s requests near the proximity of users to reduce response time for latency-sensitive requests. Despite its advantages, the properties such as resource heterogeneity and limitations, and its dynamic and unpredictable nature greatly reduce the eﬃciency of fog computing. Therefore, predicting the dynamic behavior of the fog and managing resources accordingly is of utmost importance. In this work, we provide a review of machine learning-based predictive resource management approaches in a fog environment. Resource management is classiﬁed into six sub-areas: resource provisioning, application placement, scheduling, resource allocation, task oﬄoading, and load balancing. Reviewed resource management approaches are analyzed based on the objective metrics, tools, datasets, and utilized techniques.},
	language = {en},
	urldate = {2024-12-01},
	publisher = {arXiv},
	author = {Fahimullah, Muhammad and Ahvar, Shohreh and Trocan, Maria},
	month = sep,
	year = {2022},
	note = {arXiv:2209.03066 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture, Important, Review},
	file = {PDF:/home/arthur/Zotero/storage/GNN8SIQK/Fahimullah et al. - 2022 - A Review of Resource Management in Fog Computing Machine Learning Perspective.pdf:application/pdf},
}

@article{das_review_2023,
	title = {A review on fog computing: {Issues}, characteristics, challenges, and potential applications},
	volume = {10},
	issn = {27725030},
	shorttitle = {A review on fog computing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2772503023000099},
	doi = {10.1016/j.teler.2023.100049},
	abstract = {Fog computing is a paradigm that utilizes the advantages of both the cloud and the edge devices providing quality services, reducing latency, providing mobility support, multi-tenancy, and many other functions that support modern computing systems. It is sometimes referred to as fog networking or fogging. This paper reviews and discusses cloud computing, brieﬂy highlighting the implemented paradigms before fog computing. These paradigms include cloud, mobile cloud computing, and mobile edge computing. All the paradigms targeted improving the quality of service between the end devices and the cloud itself. A fog computing Taxonomy is presented based on contemporary fog computing research about security challenges, services issues, operational issues, and data management. The standard for elucidating the taxonomy is built on the functional and vital issues in fog computing. Challenges and potential applications are identiﬁed. The review shows that security, privacy, application, and communication challenges are prominent among scholars contributions. Potential applications in fog computing are also identiﬁed, including healthcare applications, innovative city applications, and farm applications.},
	language = {en},
	urldate = {2024-12-01},
	journal = {Telematics and Informatics Reports},
	author = {Das, Resul and Inuwa, Muhammad Muhammad},
	month = jun,
	year = {2023},
	pages = {100049},
	file = {PDF:/home/arthur/Zotero/storage/IUTARJCG/Das and Inuwa - 2023 - A review on fog computing Issues, characteristics, challenges, and potential applications.pdf:application/pdf},
}

@misc{penglin_survey_2024,
	title = {A {Survey} of {Emerging} {Trends} in {Edge} {Computing}},
	url = {https://rgdoi.net/10.13140/RG.2.2.22183.76962},
	doi = {10.13140/RG.2.2.22183.76962},
	abstract = {Edge computing has emerged as a pivotal paradigm in the realm of distributed computing, promising to revolutionize the way data is processed, analyzed, and utilized at the edge of the network. This survey paper provides a comprehensive overview of the current state of edge computing, encompassing its fundamental concepts, architectural models, enabling technologies, and diverse applications across various domains. We delve into the key characteristics of edge computing, including proximity to data sources, low latency, bandwidth optimization, and scalability, highlighting its potential to address the shortcomings of centralized cloud computing. Furthermore, we examine the multitude of technologies underpinning edge computing, such as edge devices, fog computing, edge analytic, and network slicing, elucidating their roles and functionalities within the edge ecosystem. Through an extensive review of existing literature and case studies, we identify and analyze the manifold applications of edge computing in domains such as the Internet of Things (IoT), smart cities, healthcare, autonomous vehicles, and industrial automation. Additionally, we discuss the significant challenges and open research issues surrounding edge computing, including resource management, security and privacy concerns, interoperability, and standardization efforts. By synthesizing and organizing the current body of knowledge on edge computing, this survey aims to provide researchers, practitioners, and policymakers with valuable insights into the advancements, opportunities, and unresolved issues in this rapidly evolving field.},
	language = {en},
	urldate = {2024-12-01},
	publisher = {Unpublished},
	author = {Penglin, Dai and {Md Likhan}},
	year = {2024},
	file = {PDF:/home/arthur/Zotero/storage/6QD9CYDT/Penglin and Md Likhan - 2024 - A Survey of Emerging Trends in Edge Computing.pdf:application/pdf},
}

@article{zhu_cloudfog_2024,
	title = {Cloud–{Fog} {Collaborative} {Computing} {Based} {Task} {Offloading} {Strategy} in {Internet} of {Vehicles}},
	volume = {13},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/13/12/2355},
	doi = {10.3390/electronics13122355},
	abstract = {Vehicle terminals in the mobile internet of vehicles are faced with difficulty in the requirements for computation-intensive and delay-sensitive tasks, and vehicle mobility also causes dynamic changes in vehicle-to-vehicle (V2V) communication links, which results in a lower task offloading quality. To solve the above problems, a new task offloading strategy based on cloud–fog collaborative computing is proposed. Firstly, the V2V-assisted task forwarding mechanism is introduced under cloud–fog collaborative computing, and a forwarding vehicles predicting algorithm based on environmental information is designed; then, considering the parallel computing relationship of tasks in each computing node, a task offloading cost model is constructed with the goal of minimizing delay and energy consumption; finally, a multi-strategy improved genetic algorithm (MSI-GA) is proposed to solve the above task offloading optimization problem, which adapts the chaotic sequence to initialize the population, comprehensively considers the influence factors to optimize the adaptive operator, and introduces Gaussian perturbation to enhance the local optimization ability of the algorithm. The simulation experiments show that compared with the existing strategies, the proposed task offloading strategy has the lower task offloading cost for a different number of tasks and fog nodes; additionally, the introduced V2V auxiliary task forwarding mechanism can reduce the forwarding load of fog nodes by cooperative vehicles to forward tasks.},
	language = {en},
	number = {12},
	urldate = {2024-12-01},
	journal = {Electronics},
	author = {Zhu, Chunhua and Liu, Chong and Zhu, Hai and Li, Jingtao},
	month = jun,
	year = {2024},
	pages = {2355},
	file = {PDF:/home/arthur/Zotero/storage/BAW6JYEB/Zhu et al. - 2024 - Cloud–Fog Collaborative Computing Based Task Offloading Strategy in Internet of Vehicles.pdf:application/pdf},
}

@article{wang_deep_2024,
	title = {Deep {Reinforcement} {Learning}-based scheduling for optimizing system load and response time in edge and fog computing environments},
	volume = {152},
	issn = {0167739X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X23003862},
	doi = {10.1016/j.future.2023.10.012},
	abstract = {Edge/fog computing, as a distributed computing paradigm, satisfies the low-latency requirements of everincreasing number of IoT applications and has become the mainstream computing paradigm behind IoT applications. However, because large number of IoT applications require execution on the edge/fog resources, the servers may be overloaded. Hence, it may disrupt the edge/fog servers and also negatively affect IoT applications’ response time. Moreover, many IoT applications are composed of dependent components incurring extra constraints for their execution. Besides, edge/fog computing environments and IoT applications are inherently dynamic and stochastic. Thus, efficient and adaptive scheduling of IoT applications in heterogeneous edge/fog computing environments is of paramount importance. However, limited computational resources on edge/fog servers imposes an extra burden for applying optimal but computationally demanding techniques. To overcome these challenges, we propose a Deep Reinforcement Learning-based IoT application Scheduling algorithm, called DRLIS to adaptively and efficiently optimize the response time of heterogeneous IoT applications and balance the load of the edge/fog servers. We implemented DRLIS as a practical scheduler in the FogBus2 function-as-a-service framework for creating an edge–fog–cloud integrated serverless computing environment. Results obtained from extensive experiments show that DRLIS significantly reduces the execution cost of IoT applications by up to 55\%, 37\%, and 50\% in terms of load balancing, response time, and weighted cost, respectively, compared with metaheuristic algorithms and other reinforcement learning techniques.},
	language = {en},
	urldate = {2024-12-01},
	journal = {Future Generation Computer Systems},
	author = {Wang, Zhiyu and Goudarzi, Mohammad and Gong, Mingming and Buyya, Rajkumar},
	month = mar,
	year = {2024},
	keywords = {Important, DRL},
	pages = {55--69},
	file = {PDF:/home/arthur/Zotero/storage/95H36KFI/Wang et al. - 2024 - Deep Reinforcement Learning-based scheduling for optimizing system load and response time in edge an.pdf:application/pdf},
}

@article{javaid_intelligent_2019,
	title = {Intelligent {Resource} {Allocation} in {Residential} {Buildings} {Using} {Consumer} to {Fog} to {Cloud} {Based} {Framework}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1996-1073},
	url = {https://www.mdpi.com/1996-1073/12/5/815},
	doi = {10.3390/en12050815},
	abstract = {In this work, a new orchestration of Consumer to Fog to Cloud (C2F2C) based framework is proposed for efﬁciently managing the resources in residential buildings. C2F2C is a three layered framework consisting of cloud layer, fog layer and consumer layer. Cloud layer deals with on-demand delivery of the consumer’s demands. Resource management is intelligently done through the fog layer because it reduces the latency and enhances the reliability of cloud. Consumer layer is based on the residential users and their electricity demands from the six regions of the world. These regions are categorized on the bases of the continents. Two control parameters are considered: clusters of buildings and load requests, whereas four performance parameters are considered: Request Per Hour (RPH), Response Time (RT), Processing Time (PT) and cost in terms of Virtual Machines (VMs), Microgrids (MGs) and data transfer. These parameters are analysed by the round robin algorithm, equally spread current execution algorithm and our proposed algorithm shortest job ﬁrst. Two scenarios are used in the simulations: resource allocation using MGs and resource allocation using MGs and power storage devices for checking the effectiveness of the proposed work. The simulation results of the proposed technique show that it has outperformed the previous techniques in terms of the above-mentioned parameters. There exists a tradeoff in the PT and RT as compared to cost of VM, MG and data transfer.},
	language = {en},
	number = {5},
	urldate = {2024-12-01},
	journal = {Energies},
	author = {Javaid, Sakeena and Javaid, Nadeem and Saba, Tanzila and Wadud, Zahid and Rehman, Amjad and Haseeb, Abdul},
	month = mar,
	year = {2019},
	pages = {815},
	file = {PDF:/home/arthur/Zotero/storage/WWQXKWKT/Javaid et al. - 2019 - Intelligent Resource Allocation in Residential Buildings Using Consumer to Fog to Cloud Based Framew.pdf:application/pdf},
}

@article{aazam_offloading_2018,
	title = {Offloading in fog computing for {IoT}: {Review}, enabling technologies, and research opportunities},
	volume = {87},
	issn = {0167739X},
	shorttitle = {Offloading in fog computing for {IoT}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X18301973},
	doi = {10.1016/j.future.2018.04.057},
	language = {en},
	urldate = {2024-12-01},
	journal = {Future Generation Computer Systems},
	author = {Aazam, Mohammad and Zeadally, Sherali and Harras, Khaled A.},
	month = oct,
	year = {2018},
	pages = {278--289},
	file = {PDF:/home/arthur/Zotero/storage/XFTAE6LU/Aazam et al. - 2018 - Offloading in fog computing for IoT Review, enabling technologies, and research opportunities.pdf:application/pdf},
}

@article{yuan_online_2021,
	title = {Online {Dispatching} and {Fair} {Scheduling} of {Edge} {Computing} {Tasks}: {A} {Learning}-{Based} {Approach}},
	volume = {8},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4662, 2372-2541},
	shorttitle = {Online {Dispatching} and {Fair} {Scheduling} of {Edge} {Computing} {Tasks}},
	url = {https://ieeexplore.ieee.org/document/9403379/},
	doi = {10.1109/JIOT.2021.3073034},
	abstract = {The emergence of edge computing can effectively tackle the problem of large transmission delays caused by the long-distance between user devices and remote cloud servers. Users can ofﬂoad tasks to the nearby edge servers to perform computations, so as to minimize the average task response time through effective task dispatching and scheduling methods. However, i) in the task dispatching phase, the dynamic features of network conditions and server loads make it difﬁcult for the ofﬂoaded tasks to select the optimal edge server, and ii) in the task scheduling phase, each edge server may face a large number of ofﬂoading tasks to schedule, resulting in long average task response time, or even severe task starvation. In this paper, we propose an online task dispatching and fair scheduling method OTDS to tackle the above two challenges, which combines online learning and deep reinforcement learning techniques. Speciﬁcally, using an online learning approach, OTDS performs real-time estimating of network conditions and server loads, and then dynamically assigns tasks to the optimal edge servers accordingly. Meanwhile, at each edge server, by combing the round-robin (RR) mechanism with deep reinforcement learning (DRL), OTDS is able to allocate appropriate resources to each task according to its time-sensitivity and achieve high efﬁciency and fairness in task scheduling. Evaluation results show that our online method can dynamically allocate network resources and computing resources to those ofﬂoaded tasks according to their time-sensitive requirements. Thus OTDS outperforms the existing methods in terms of the efﬁciency and fairness on task dispatching and scheduling by signiﬁcantly reducing the average task response time.},
	language = {en},
	number = {19},
	urldate = {2024-12-01},
	journal = {IEEE Internet of Things Journal},
	author = {Yuan, Hao and Tang, Guoming and Li, Xinyi and Guo, Deke and Luo, Lailong and Luo, Xueshan},
	month = oct,
	year = {2021},
	pages = {14985--14998},
	file = {PDF:/home/arthur/Zotero/storage/HERDK97Z/Yuan et al. - 2021 - Online Dispatching and Fair Scheduling of Edge Computing Tasks A Learning-Based Approach.pdf:application/pdf},
}

@article{jiang_reinforcement_2021,
	title = {A reinforcement learning-based computing offloading and resource allocation scheme in {F}-{RAN}},
	volume = {2021},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-021-00802-x},
	doi = {10.1186/s13634-021-00802-x},
	abstract = {This paper investigates a computing offloading policy and the allocation of computational resource for multiple user equipments (UEs) in device-to-device (D2D)-aided fog radio access networks (F-RANs). Concerning the dynamically changing wireless environment where the channel state information (CSI) is difficult to predict and know exactly, we formulate the problem of task offloading and resource optimization as a mixed-integer nonlinear programming problem to maximize the total utility of all UEs. Concerning the non-convex property of the formulated problem, we decouple the original problem into two phases to solve. Firstly, a centralized deep reinforcement learning (DRL) algorithm called dueling deep Q-network (DDQN) is utilized to obtain the most suitable offloading mode for each UE. Particularly, to reduce the complexity of the proposed offloading scheme-based DDQN algorithm, a pre-processing procedure is adopted. Then, a distributed deep Q-network (DQN) algorithm based on the training result of the DDQN algorithm is further proposed to allocate the appropriate computational resource for each UE. Combining these two phases, the optimal offloading policy and resource allocation for each UE are finally achieved. Simulation results demonstrate the performance gains of the proposed scheme compared with other existing baseline schemes.},
	language = {en},
	number = {1},
	urldate = {2024-12-01},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Jiang, Fan and Ma, Rongxin and Gao, Youjun and Gu, Zesheng},
	month = dec,
	year = {2021},
	pages = {91},
	file = {PDF:/home/arthur/Zotero/storage/KVP2UYN7/Jiang et al. - 2021 - A reinforcement learning-based computing offloading and resource allocation scheme in F-RAN.pdf:application/pdf},
}

@article{tu_task_2022,
	title = {Task {Offloading} {Based} on {LSTM} {Prediction} and {Deep} {Reinforcement} {Learning} for {Efficient} {Edge} {Computing} in {IoT}},
	volume = {14},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1999-5903},
	url = {https://www.mdpi.com/1999-5903/14/2/30},
	doi = {10.3390/fi14020030},
	abstract = {In IoT (Internet of Things) edge computing, task ofﬂoading can lead to additional transmission delays and transmission energy consumption. To reduce the cost of resources required for task ofﬂoading and improve the utilization of server resources, in this paper, we model the task ofﬂoading problem as a joint decision making problem for cost minimization, which integrates the processing latency, processing energy consumption, and the task throw rate of latency-sensitive tasks. The Online Predictive Ofﬂoading (OPO) algorithm based on Deep Reinforcement Learning (DRL) and Long Short-Term Memory (LSTM) networks is proposed to solve the above task ofﬂoading decision problem. In the training phase of the model, this algorithm predicts the load of the edge server in real-time with the LSTM algorithm, which effectively improves the convergence accuracy and convergence speed of the DRL algorithm in the ofﬂoading process. In the testing phase , the LSTM network is used to predict the characteristics of the next task, and then the computational resources are allocated for the task in advance by the DRL decision model, thus further reducing the response delay of the task and enhancing the ofﬂoading performance of the system. The experimental evaluation shows that this algorithm can effectively reduce the average latency by 6.25\%, the ofﬂoading cost by 25.6\%, and the task throw rate by 31.7\%.},
	language = {en},
	number = {2},
	urldate = {2024-12-01},
	journal = {Future Internet},
	author = {Tu, Youpeng and Chen, Haiming and Yan, Linjie and Zhou, Xinyan},
	month = jan,
	year = {2022},
	pages = {30},
	file = {PDF:/home/arthur/Zotero/storage/GQKXBIFB/Tu et al. - 2022 - Task Offloading Based on LSTM Prediction and Deep Reinforcement Learning for Efficient Edge Computin.pdf:application/pdf},
}

@misc{gholipour_tpto_2023,
	title = {{TPTO}: {A} {Transformer}-{PPO} based {Task} {Offloading} {Solution} for {Edge} {Computing} {Environments}},
	shorttitle = {{TPTO}},
	url = {http://arxiv.org/abs/2312.11739},
	doi = {10.48550/arXiv.2312.11739},
	abstract = {Emerging applications in healthcare, autonomous vehicles, and wearable assistance require interactive and low-latency data analysis services. Unfortunately, cloud-centric architectures cannot fulfill the low-latency demands of these applications, as user devices are often distant from cloud data centers. Edge computing aims to reduce the latency by enabling processing tasks to be offloaded to resources located at the network's edge. However, determining which tasks must be offloaded to edge servers to reduce the latency of application requests is not trivial, especially if the tasks present dependencies. This paper proposes a DRL approach called TPTO, which leverages Transformer Networks and PPO to offload dependent tasks of IoT applications in edge computing. We consider users with various preferences, where devices can offload computation to an edge server via wireless channels. Performance evaluation results demonstrate that under fat application graphs, TPTO is more effective than state-of-the-art methods, such as Greedy, HEFT, and MRLCO, by reducing latency by 30.24\%, 29.61\%, and 12.41\%, respectively. In addition, TPTO presents a training time approximately 2.5 times faster than an existing DRL approach.},
	language = {en},
	urldate = {2024-12-01},
	publisher = {arXiv},
	author = {Gholipour, Niloofar and Assuncao, Marcos Dias de and Agarwal, Pranav and gascon-samson, julien and Buyya, Rajkumar},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11739 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {PDF:/home/arthur/Zotero/storage/E2UYXWD7/Gholipour et al. - 2023 - TPTO A Transformer-PPO based Task Offloading Solution for Edge Computing Environments.pdf:application/pdf},
}

@article{bukhari_intelligent_2022,
	title = {An {Intelligent} {Proposed} {Model} for {Task} {Offloading} in {Fog}-{Cloud} {Collaboration} {Using} {Logistics} {Regression}},
	volume = {2022},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1687-5273, 1687-5265},
	url = {https://www.hindawi.com/journals/cin/2022/3606068/},
	doi = {10.1155/2022/3606068},
	abstract = {Smart applications and intelligent systems are being developed that are self-reliant, adaptive, and knowledge-based in nature. Emergency and disaster management, aerospace, healthcare, IoT, and mobile applications, among them, revolutionize the world of computing. Applications with a large number of growing devices have transformed the current design of centralized cloud impractical. Despite the use of 5G technology, delay-sensitive applications and cloud cannot go parallel due to exceeding threshold values of certain parameters like latency, bandwidth, response time, etc. Middleware proves to be a better solution to cope up with these issues while satisfying the high requirements task offloading standards. Fog computing is recommended middleware in this research article in view of the fact that it provides the services to the edge of the network; delay-sensitive applications can be entertained effectively. On the contrary, fog nodes contain a limited set of resources that may not process all tasks, especially of computation-intensive applications. Additionally, fog is not the replacement of the cloud, rather supplement to the cloud, both behave like counterparts and offer their services correspondingly to compliance the task needs but fog computing has relatively closer proximity to the devices comparatively cloud. The problem arises when a decision needs to take what is to be offloaded: data, computation, or application, and more specifically where to offload: either fog or cloud and how much to offload. Fog-cloud collaboration is stochastic in terms of task-related attributes like task size, duration, arrival rate, and required resources. Dynamic task offloading becomes crucial in order to utilize the resources at fog and cloud to improve QoS. Since this formation of task offloading policy is a bit complex in nature, this problem is addressed in the research article and proposes an intelligent task offloading model. Simulation results demonstrate the authenticity of the proposed logistic regression model acquiring 86\% accuracy compared to other algorithms and confidence in the predictive task offloading policy by making sure process consistency and reliability.},
	language = {en},
	urldate = {2024-12-02},
	journal = {Computational Intelligence and Neuroscience},
	author = {Bukhari, Muhammad Mazhar and Ghazal, Taher M. and Abbas, Sagheer and Khan, M. A. and Farooq, Umer and Wahbah, Hasan and Ahmad, Munir and Adnan, Khan Muhammad},
	editor = {Travieso-González, Carlos M.},
	month = jan,
	year = {2022},
	pages = {1--25},
	file = {PDF:/home/arthur/Zotero/storage/WLXU7HUL/Bukhari et al. - 2022 - An Intelligent Proposed Model for Task Offloading in Fog-Cloud Collaboration Using Logistics Regress.pdf:application/pdf},
}

@article{bukhari_intelligent_2022-1,
	title = {An {Intelligent} {Proposed} {Model} for {Task} {Offloading} in {Fog}-{Cloud} {Collaboration} {Using} {Logistics} {Regression}},
	volume = {2022},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1687-5273, 1687-5265},
	url = {https://www.hindawi.com/journals/cin/2022/3606068/},
	doi = {10.1155/2022/3606068},
	abstract = {Smart applications and intelligent systems are being developed that are self-reliant, adaptive, and knowledge-based in nature. Emergency and disaster management, aerospace, healthcare, IoT, and mobile applications, among them, revolutionize the world of computing. Applications with a large number of growing devices have transformed the current design of centralized cloud impractical. Despite the use of 5G technology, delay-sensitive applications and cloud cannot go parallel due to exceeding threshold values of certain parameters like latency, bandwidth, response time, etc. Middleware proves to be a better solution to cope up with these issues while satisfying the high requirements task offloading standards. Fog computing is recommended middleware in this research article in view of the fact that it provides the services to the edge of the network; delay-sensitive applications can be entertained effectively. On the contrary, fog nodes contain a limited set of resources that may not process all tasks, especially of computation-intensive applications. Additionally, fog is not the replacement of the cloud, rather supplement to the cloud, both behave like counterparts and offer their services correspondingly to compliance the task needs but fog computing has relatively closer proximity to the devices comparatively cloud. The problem arises when a decision needs to take what is to be offloaded: data, computation, or application, and more specifically where to offload: either fog or cloud and how much to offload. Fog-cloud collaboration is stochastic in terms of task-related attributes like task size, duration, arrival rate, and required resources. Dynamic task offloading becomes crucial in order to utilize the resources at fog and cloud to improve QoS. Since this formation of task offloading policy is a bit complex in nature, this problem is addressed in the research article and proposes an intelligent task offloading model. Simulation results demonstrate the authenticity of the proposed logistic regression model acquiring 86\% accuracy compared to other algorithms and confidence in the predictive task offloading policy by making sure process consistency and reliability.},
	language = {en},
	urldate = {2024-12-05},
	journal = {Computational Intelligence and Neuroscience},
	author = {Bukhari, Muhammad Mazhar and Ghazal, Taher M. and Abbas, Sagheer and Khan, M. A. and Farooq, Umer and Wahbah, Hasan and Ahmad, Munir and Adnan, Khan Muhammad},
	editor = {Travieso-González, Carlos M.},
	month = jan,
	year = {2022},
	pages = {1--25},
	file = {PDF:/home/arthur/Zotero/storage/PSRPU2LP/Bukhari et al. - 2022 - An Intelligent Proposed Model for Task Offloading in Fog-Cloud Collaboration Using Logistics Regress.pdf:application/pdf},
}

@article{ometov_survey_2022,
	title = {A {Survey} of {Security} in {Cloud}, {Edge}, and {Fog} {Computing}},
	volume = {22},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/3/927},
	doi = {10.3390/s22030927},
	abstract = {The ﬁeld of information security and privacy is currently attracting a lot of research interest. Simultaneously, different computing paradigms from Cloud computing to Edge computing are already forming a unique ecosystem with different architectures, storage, and processing capabilities. The heterogeneity of this ecosystem comes with certain limitations, particularly security and privacy challenges. This systematic literature review aims to identify similarities, differences, main attacks, and countermeasures in the various paradigms mentioned. The main determining outcome points out the essential security and privacy threats. The presented results also outline important similarities and differences in Cloud, Edge, and Fog computing paradigms. Finally, the work identiﬁed that the heterogeneity of such an ecosystem does have issues and poses a great setback in the deployment of security and privacy mechanisms to counter security attacks and privacy leakages. Different deployment techniques were found in the review studies as ways to mitigate and enhance security and privacy shortcomings.},
	language = {en},
	number = {3},
	urldate = {2024-12-05},
	journal = {Sensors},
	author = {Ometov, Aleksandr and Molua, Oliver and Komarov, Mikhail and Nurmi, Jari},
	month = jan,
	year = {2022},
	pages = {927},
	file = {PDF:/home/arthur/Zotero/storage/UHDHZGGD/Ometov et al. - 2022 - A Survey of Security in Cloud, Edge, and Fog Computing.pdf:application/pdf},
}

@article{costa_orchestration_2023,
	title = {Orchestration in {Fog} {Computing}: {A} {Comprehensive} {Survey}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Orchestration in {Fog} {Computing}},
	url = {https://dl.acm.org/doi/10.1145/3486221},
	doi = {10.1145/3486221},
	abstract = {Fog computing is a paradigm that brings computational resources and services to the network edge in the vicinity of user devices, lowering latency and connecting with cloud computing resources. Unlike cloud computing, fog resources are based on constrained and heterogeneous nodes whose connectivity can be unstable. In this complex scenario, there is a need to define and implement orchestration processes to ensure that applications and services can be provided, considering the settled agreements. Although some publications have dealt with orchestration in fog computing, there are still some diverse definitions and functional intersection with other areas, such as resource management and monitoring. This article presents a systematic review of the literature with focus on orchestration in fog computing. A generic architecture of fog orchestration is presented, created from the consolidation of the analyzed proposals, bringing to light the essential functionalities addressed in the literature. This work also highlights the main challenges and open research questions.},
	language = {en},
	number = {2},
	urldate = {2024-12-05},
	journal = {ACM Computing Surveys},
	author = {Costa, Breno and Bachiega, Joao and De Carvalho, Leonardo Rebouças and Araujo, Aleteia P. F.},
	month = feb,
	year = {2023},
	pages = {1--34},
	file = {PDF:/home/arthur/Zotero/storage/XRLPU2GE/Costa et al. - 2023 - Orchestration in Fog Computing A Comprehensive Survey.pdf:application/pdf},
}

@article{wang_integration_nodate,
	title = {Integration of {FogBus2} {Framework} with {Container} {Orchestration} {Tools} in {Cloud} and {Edge} {Computing} {Environments}},
	abstract = {Currently, due to the advantages of light weight, simple deployment, multienvironment support, short startup time, scalability, and easy migration, container technology has been widely used in both cloud and edge/fog computing, and addresses the problem of device heterogeneity in different computing environments. On this basis, as one of the most popular container orchestration and management systems, Kubernetes almost dominates the cloud environment. However, since it is primarily designed for centralized resource management scenarios where computing resources are sufficient, the system is unstable in edge environments due to hardware limitations. Therefore, in order to realize container orchestration in the cloud and edge/fog hybrid computing environment, we propose a feasible approach to build a hybrid clustering based on K3s, which solves the problem that virtual instances in different environments cannot be connected due to IP addresses. We also propose three design patterns for deploying the FogBus2 framework into hybrid environments, including 1) Host Network Mode, 2) Proxy Server, and 3) Environment Variable.},
	language = {en},
	author = {Wang, Zhiyu},
	file = {PDF:/home/arthur/Zotero/storage/I2ABC9PC/Wang - Integration of FogBus2 Framework with Container Orchestration Tools in Cloud and Edge Computing Envi.pdf:application/pdf},
}

@article{wali_anomaly_nodate,
	title = {Anomaly {Detection} in {Fog} {Computing}: {State}-of-the-{Art} {Techniques}, applications, {Challenges}, and {Future} {Directions}},
	abstract = {The fog computing provides a platform for various time critical real-time applications. It reduces the communication latency by placing computational resources to near to IoT devices. The anomaly detection in fog computing is very essential aspect to improve the quality of service. An anomaly is a value, a status of resources or outcome that deviates from expected or normal values which affects the Quality of services. Anomaly detection is an important data analysis task which is useful for identifying various issues in fog computing environment. This paper presents detailed study of anomaly detection key factors, general architecture and anomaly detection techniques. It describes the state of the art anomaly detection techniques with comparative analysis. Finally, various applications and challenges/issues with future directions are discussed. This paper helps researchers, engineers and scientists to know the state-of-the-art works for anomaly detection techniques in fog computing environment.},
	language = {en},
	author = {Wali, Girish and Bulla, Dr Chetan},
	file = {Autonomous computation ofoading and auto‑scaling:/home/arthur/Zotero/storage/XRJDU6KD/Autonomous computation ofoading and auto‑scaling.pdf:application/pdf;PDF:/home/arthur/Zotero/storage/5KK67RH5/Wali and Bulla - Anomaly Detection in Fog Computing State-of-the-Art Techniques, applications, Challenges, and Futur.pdf:application/pdf},
}

@misc{pakmehr_task_2024,
	title = {Task {Offloading} in {Fog} {Computing} with {Deep} {Reinforcement} {Learning}: {Future} {Research} {Directions} {Based} on {Security} and {Efficiency} {Enhancements}},
	shorttitle = {Task {Offloading} in {Fog} {Computing} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2407.19121},
	doi = {10.48550/arXiv.2407.19121},
	abstract = {The surge in Internet of Things (IoT) devices and data generation highlights the limitations of traditional cloud computing in meeting demands for immediacy, Quality of Service, and location-aware services. Fog computing emerges as a solution, bringing computation, storage, and networking closer to data sources. This study explores the role of Deep Reinforcement Learning in enhancing fog computing’s task offloading, aiming for operational efficiency and robust security. By reviewing current strategies and proposing future research directions, the paper shows the potential of Deep Reinforcement Learning in optimizing resource use, speeding up responses, and securing against vulnerabilities. It suggests advancing Deep Reinforcement Learning for fog computing, exploring blockchain for better security, and seeking energy-efficient models to improve the Internet of Things ecosystem. Incorporating artificial intelligence, Our results indicate potential improvements in key metrics, such as task completion time, energy consumption, and security incident reduction. These findings provide a concrete foundation for future research and practical applications in optimizing fog computing architectures.},
	language = {en},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Pakmehr, Amir},
	month = jul,
	year = {2024},
	note = {arXiv:2407.19121 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {PDF:/home/arthur/Zotero/storage/JSNCYIMC/Pakmehr - 2024 - Task Offloading in Fog Computing with Deep Reinforcement Learning Future Research Directions Based.pdf:application/pdf},
}

@article{jiang_delay-aware_2018,
	title = {Delay-{Aware} {Task} {Offloading} in {Shared} {Fog} {Networks}},
	volume = {5},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4662, 2372-2541},
	url = {https://ieeexplore.ieee.org/document/8527523/},
	doi = {10.1109/JIOT.2018.2880250},
	abstract = {Ofﬂoading computation tasks from resource-poor end devices to powerful backend clouds has become a prevalent solution thanks to the rapid development of cloud computing. However, modern Internet of Things applications, such as augmented reality and real-time monitoring, bring stringent delay requirements to the computation tasks in the deviceto-computing-facility communications. To better accommodate the delay requirements of the computation tasks, the recently proposed fog computing concept suggests that these computation tasks can be extensively ofﬂoaded to the distributed computation facilities along the cloud-to-things continuum. These computation facilities, including central clouds and the computation facilities standing at the network edge, jointly form an overlay network, named a fog network, to provide fog computing services for end devices. This paper targets a practical and efﬁcient scheme to schedule tasks with heterogeneous delay sensitivities in a shared fog network. A mathematical model is ﬁrst constructed to capture the major characteristics of a fog network. The model enforces lexicographic max-min fairness, an enhanced metric compared to conventional max-min fairness. The task ofﬂoading problem is modeled as an integer non-linear program. An efﬁcient and exact solution method is proposed based on problem-speciﬁc analysis. Finally, synthesized-trace-driven simulations demonstrate the efﬁcacy of our proposed ofﬂoading scheme.},
	language = {en},
	number = {6},
	urldate = {2024-12-05},
	journal = {IEEE Internet of Things Journal},
	author = {Jiang, Yuxuan and Tsang, Danny H. K.},
	month = dec,
	year = {2018},
	pages = {4945--4956},
	file = {PDF:/home/arthur/Zotero/storage/SWSVVEIT/Jiang and Tsang - 2018 - Delay-Aware Task Offloading in Shared Fog Networks.pdf:application/pdf},
}

@article{iftikhar_ai-based_2023,
	title = {{AI}-based {Fog} and {Edge} {Computing}: {A} {Systematic} {Review}, {Taxonomy} and {Future} {Directions}},
	volume = {21},
	issn = {25426605},
	shorttitle = {{AI}-based {Fog} and {Edge} {Computing}},
	url = {http://arxiv.org/abs/2212.04645},
	doi = {10.1016/j.iot.2022.100674},
	abstract = {Resource management in computing is a very challenging problem that involves making sequential decisions. Resource limitations, resource heterogeneity, dynamic and diverse nature of workload, and the unpredictability of fog/edge computing environments have made resource management even more challenging to be considered in the fog landscape. Recently Artiﬁcial Intelligence (AI) and Machine Learning (ML) based solutions are adopted to solve this problem. AI/ML methods with the capability to make sequential decisions like reinforcement learning seem most promising for these type of problems. But these algorithms come with their own challenges such as high variance, explainability, and online training. The continuously changing fog/edge environment dynamics require solutions that learn online, adopting changing computing environment. In this paper, we used standard review methodology to conduct this Systematic Literature Review (SLR) to analyze the role of AI/ML algorithms and the challenges in the applicability of these algorithms for resource management in fog/edge computing environments. Further, various machine learning, deep learning and reinforcement learning techniques for edge AI management have been discussed. Furthermore, we have presented the background and current status of AI/ML-based Fog/Edge Computing. Moreover, a taxonomy of AI/ML-based resource management techniques for fog/edge computing has been proposed and compared the existing techniques based on the proposed taxonomy. Finally, open challenges and promising future research directions have been identiﬁed and discussed in the area of AI/ML-based fog/edge computing.},
	language = {en},
	urldate = {2024-12-05},
	journal = {Internet of Things},
	author = {Iftikhar, Sundas and Gill, Sukhpal Singh and Song, Chenghao and Xu, Minxian and Aslanpour, Mohammad Sadegh and Toosi, Adel N. and Du, Junhui and Wu, Huaming and Ghosh, Shreya and Chowdhury, Deepraj and Golec, Muhammed and Kumar, Mohit and Abdelmoniem, Ahmed M. and Cuadrado, Felix and Varghese, Blesson and Rana, Omer and Dustdar, Schahram and Uhlig, Steve},
	month = apr,
	year = {2023},
	note = {arXiv:2212.04645 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {100674},
	file = {PDF:/home/arthur/Zotero/storage/9XQR7GHD/Iftikhar et al. - 2023 - AI-based Fog and Edge Computing A Systematic Review, Taxonomy and Future Directions.pdf:application/pdf},
}

@article{guerrero_genetic-based_2022,
	title = {Genetic-based optimization in {Fog} {Computing}: current trends and research opportunities},
	volume = {72},
	issn = {22106502},
	shorttitle = {Genetic-based optimization in {Fog} {Computing}},
	url = {http://arxiv.org/abs/2112.01958},
	doi = {10.1016/j.swevo.2022.101094},
	abstract = {Fog computing is a new computational paradigm that emerged from the need to reduce network usage and latency in the Internet of Things (IoT). Fog can be considered as a continuum between the cloud layer and IoT users that allows the execution of applications or storage/processing of data in network infrastructure devices. The heterogeneity and wider distribution of fog devices are the key differences between cloud and fog infrastructure. Genetic-based optimization is commonly used in distributed systems; however, the differentiating features of fog computing require new designs, studies, and experimentation. The growing research in the field of genetic-based fog resource optimization and the lack of previous analysis in this field have encouraged us to present a comprehensive, exhaustive, and systematic review of the most recent research works. Resource optimization techniques in fog were examined and analyzed, with special emphasis on genetic-based solutions and their characteristics and design alternatives. We defined a classification of the optimization scope in fog infrastructures and used this optimization taxonomy to classify the 70 papers in this survey. Subsequently, the papers were assessed in terms of genetic optimization design. Finally, the benefits and limitations of each surveyed work are outlined in this paper. Based on these previous analyses of the relevant literature, future research directions were identified. We concluded that more research efforts are needed to address the current challenges in data management, workflow scheduling, and service placement. Additionally, there is still room for improved designs and deployments of parallel and hybrid genetic algorithms that leverage, and adapt to, the heterogeneity and distributed features of fog domains.},
	language = {en},
	urldate = {2024-12-05},
	journal = {Swarm and Evolutionary Computation},
	author = {Guerrero, Carlos and Lera, Isaac and Juiz, Carlos},
	month = jul,
	year = {2022},
	note = {arXiv:2112.01958 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {101094},
	file = {PDF:/home/arthur/Zotero/storage/C3K6WWRX/Guerrero et al. - 2022 - Genetic-based optimization in Fog Computing current trends and research opportunities.pdf:application/pdf},
}

@article{patel_navigating_nodate,
	title = {Navigating {Fog} {Federation}: {Classifying} {Current} {Research} and {Identifying} {Challenges}.},
	language = {en},
	author = {Patel, Dhairya},
	file = {PDF:/home/arthur/Zotero/storage/M3D7CE6J/Patel - Dhairya Patela and Shaifali P. Malukanib.pdf:application/pdf},
}

@article{aazam_cloud_2022,
	title = {Cloud of {Things} ({CoT}): {Cloud}-{Fog}-{IoT} {Task} {Offloading} for {Sustainable} {Internet} of {Things}},
	volume = {7},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2377-3782, 2377-3790},
	shorttitle = {Cloud of {Things} ({CoT})},
	url = {https://ieeexplore.ieee.org/document/9214500/},
	doi = {10.1109/TSUSC.2020.3028615},
	abstract = {With a high rise in the popularity of Internet of Things (IoT), mobile computing, and wearable devices, a huge amount of data is being generated. Running complex tasks such as that are machine learning-based with minimum energy consumption is a challenge. It requires complex algorithms to run locally such as on middleware fog within the proximity of the devices generating data, or globally in a cloud to analyze the acquired data and create robust and smart applications. However, it depends on the type of task execution policy applied at each level; local or global, to decide on energy and performance efﬁciency, since certain tasks are high in complexity. Hence, task execution will be hierarchically distributed among the IoT nodes, fog, and cloud. Given that, we present in this paper a three-tier IoT-fog-cloud model. We argue that with distributed task execution, we can achieve high scalability of IoT services, and manage the global energy consumption as well. As a proof-of-concept, we evaluate our three-tier architecture by taking into account computational tasks for various applications in IoT related to medical, multimedia, location-based, and text. We evaluate using real datasets, based on three scenarios: fog-only, cloud-only, and fog-cloud collaborative. Task execution policy (at fog/cloud) play a key role in efﬁciently processing a task (especially large tasks, such as in deep learning). Therefore, we take that into account and elaborate what types of policies suit what type of ofﬂoading environment (fog-only, cloud-only, or fog-cloud collaborative).},
	language = {en},
	number = {1},
	urldate = {2024-12-05},
	journal = {IEEE Transactions on Sustainable Computing},
	author = {Aazam, Mohammad and Islam, Saif Ul and Lone, Salman Tariq and Abbas, Assad},
	month = jan,
	year = {2022},
	pages = {87--98},
	file = {PDF:/home/arthur/Zotero/storage/72XKIWKY/Aazam et al. - 2022 - Cloud of Things (CoT) Cloud-Fog-IoT Task Offloading for Sustainable Internet of Things.pdf:application/pdf},
}

@misc{mcchesney_defog_2019,
	title = {{DeFog}: {Fog} {Computing} {Benchmarks}},
	shorttitle = {{DeFog}},
	url = {http://arxiv.org/abs/1907.10890},
	doi = {10.48550/arXiv.1907.10890},
	abstract = {Fog computing envisions that deploying services of an application across resources in the cloud and those located at the edge of the network may improve the overall performance of the application when compared to running the application on the cloud. However, there are currently no benchmarks that can directly compare the performance of the application across the cloud-only, edgeonly and cloud-edge deployment platform to obtain any insight on performance improvement. This paper proposes DeFog, a first Fog benchmarking suite to: (i) alleviate the burden of Fog benchmarking by using a standard methodology, and (ii) facilitate the understanding of the target platform by collecting a catalogue of relevant metrics for a set of benchmarks. The current portfolio of DeFog benchmarks comprises six relevant applications conducive to using the edge. Experimental studies are carried out on multiple target platforms to demonstrate the use of DeFog for collecting metrics related to application latencies (communication and computation), for understanding the impact of stress and concurrent users on application latencies, and for understanding the performance of deploying different combination of services of an application across the cloud and edge. DeFog is available for public download (https://github.com/qub-blesson/DeFog).},
	language = {en},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {McChesney, Jonathan and Wang, Nan and Tanwer, Ashish and Lara, Eyal de and Varghese, Blesson},
	month = jul,
	year = {2019},
	note = {arXiv:1907.10890 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {PDF:/home/arthur/Zotero/storage/J2ZEPAN4/McChesney et al. - 2019 - DeFog Fog Computing Benchmarks.pdf:application/pdf},
}

@article{ren_deep_2021,
	title = {Deep {Reinforcement} {Learning} {Based} {Computation} {Offloading} in {Fog} {Enabled} {Industrial} {Internet} of {Things}},
	volume = {17},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/9183960/},
	doi = {10.1109/TII.2020.3021024},
	abstract = {Fog computing is seen as a key enabler to meet the stringent requirements of Industrial Internet of Things (IIoT). Speciﬁcally, lower latency and IIoT devices’ energy consumption can be achieved by ofﬂoading computation-intensive tasks to fog access points (F-APs). However, traditional computation ofﬂoading optimization methods often possess high complexity, making them inapplicable in practical IIoT. To overcome this issue, this paper proposes a deep reinforcement learning (DRL) based approach to minimize long-term system energy consumption in a computation ofﬂoading scenario with multiple IIoT devices and multiple F-APs. The proposal features a multi-agent setting to deal with the curse of dimensionality of the action space by creating a DRL model for each IIoT device, which identiﬁes its serving F-AP based on network and device states. After FAP selection is ﬁnished, a low complexity greedy algorithm is executed at each F-AP under a computation capability constraint to determine which ofﬂoading requests are further forwarded to the cloud. By conducting ofﬂine training in the cloud and then making decisions online, iterative online optimization procedures are avoided and hence F-APs can quickly adjust F-AP selection for each device with trained DRL models. Via simulation, the impact of batch size on system performance is demonstrated and the proposed DRL based approach shows competitive performance compared to various baselines including exhaustive search and genetic algorithm based approaches. In addition, the generalization capability of the proposal is veriﬁed as well.},
	language = {en},
	number = {7},
	urldate = {2025-01-04},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Ren, Yijing and Sun, Yaohua and Peng, Mugen},
	month = jul,
	year = {2021},
	pages = {4978--4987},
	file = {PDF:/home/arthur/Zotero/storage/TZDBBFGA/Ren et al. - 2021 - Deep Reinforcement Learning Based Computation Offloading in Fog Enabled Industrial Internet of Thing.pdf:application/pdf},
}

@article{suryadevara_energy_2021,
	title = {Energy and latency reductions at the fog gateway using a machine learning classifier},
	volume = {31},
	issn = {22105379},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210537921000731},
	doi = {10.1016/j.suscom.2021.100582},
	abstract = {Machine Learning (ML) techniques have changed the analysis of massive data in the Internet of Things (IoT) environment very effectively. In the IoT theme of applications, reducing latency and energy consumption are the two crucial network Quality of Service (QoS) parameters and the most significant challenges because they directly impact the users’ experience. Enabling intelligence at the IoT fog computing framework with ML classifiers’ help determines the computing requirements that, in turn, help to execute the vast data collected in the IoT fog computing for real-time operations efficiently. In this paper, the exploration of ML algorithms on the resource constraint IoT fog computing framework and the determination of the suitable ML classifier for reducing latency and energy levels with the usage of ambient sensors in the IoT theme are presented.},
	language = {en},
	urldate = {2025-01-06},
	journal = {Sustainable Computing: Informatics and Systems},
	author = {Suryadevara, Nagender Kumar},
	month = sep,
	year = {2021},
	pages = {100582},
	file = {PDF:/home/arthur/Zotero/storage/2INPYX3W/Suryadevara - 2021 - Energy and latency reductions at the fog gateway using a machine learning classifier.pdf:application/pdf},
}

@article{adhikari_dpto_2020,
	title = {{DPTO}: {A} {Deadline} and {Priority}-{Aware} {Task} {Offloading} in {Fog} {Computing} {Framework} {Leveraging} {Multilevel} {Feedback} {Queueing}},
	volume = {7},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4662, 2372-2541},
	shorttitle = {{DPTO}},
	url = {https://ieeexplore.ieee.org/document/8863944/},
	doi = {10.1109/JIOT.2019.2946426},
	abstract = {By providing ﬂexible and shared computing and communication resources along with cloud services, fog computing became an attractive paradigm to support delay-sensitive tasks in Internet of Things (IoT). Existing researches for ofﬂoading delay-sensitive tasks in a hierarchical fog-cloud environment mostly focused on minimizing the overall communication delay. However, a fair ofﬂoading strategy selects a suitable computing device in terms of fog node or cloud server based on the resource requirements of the task while meeting the deadline. In this paper, we design a new delay-dependent priority-aware ofﬂoading (DPTO) strategy for scheduling and processing the tasks, generated from IoT devices to suitable computing devices. The proposed strategy assigns a priority on each task based on its deadline and assigns it to a suitable multilevel-feedback queue. This schema reduces the waiting time of the delay-sensitive tasks on the queue and minimizes the starvation problem of the low priority tasks. Moreover, DPTO strategy selects an optimal computing device for each task based on its resource availability and transmission time from the IoT device. This strategy minimizes the overall ofﬂoading time of the tasks while meeting the deadlines. Finally, the extensive simulation results with various performance parameters show the effectiveness of the proposed strategy over the existing baseline algorithms.},
	language = {en},
	number = {7},
	urldate = {2025-01-06},
	journal = {IEEE Internet of Things Journal},
	author = {Adhikari, Mainak and Mukherjee, Mithun and Srirama, Satish Narayana},
	month = jul,
	year = {2020},
	pages = {5773--5782},
	file = {PDF:/home/arthur/Zotero/storage/96WL4C45/Adhikari et al. - 2020 - DPTO A Deadline and Priority-Aware Task Offloading in Fog Computing Framework Leveraging Multilevel.pdf:application/pdf},
}


@article{yan_optimal_2020,
	title = {Optimal {Task} {Offloading} and {Resource} {Allocation} in {Mobile}-{Edge} {Computing} {With} {Inter}-{User} {Task} {Dependency}},
	volume = {19},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1536-1276, 1558-2248},
	url = {https://ieeexplore.ieee.org/document/8854339/},
	doi = {10.1109/TWC.2019.2943563},
	abstract = {Mobile-edge computing (MEC) has recently emerged as a cost-effective paradigm to enhance the computing capability of hardware-constrained wireless devices (WDs). In this paper, we ﬁrst consider a two-user MEC network, where each WD has a sequence of tasks to execute. In particular, we consider task dependency between the two WDs, where the input of a task at one WD requires the ﬁnal task output at the other WD. Under the considered task-dependency model, we study the optimal task ofﬂoading policy and resource allocation (e.g., on ofﬂoading transmit power and local CPU frequencies) that minimize the weighted sum of the WDs’ energy consumption and task execution time. The problem is challenging due to the combinatorial nature of the ofﬂoading decisions among all tasks and the strong coupling with resource allocation. To tackle this problem, we ﬁrst assume that the ofﬂoading decisions are given and derive the closed-form expressions of the optimal ofﬂoading transmit power and local CPU frequencies. Then, an efﬁcient bisection search method is proposed to obtain the optimal solutions. Furthermore, we prove that the optimal ofﬂoading decisions follow an one-climb policy, based on which a reduced-complexity Gibbs Sampling algorithm is proposed to obtain the optimal ofﬂoading decisions. We then extend the investigation to a general multi-user scenario, where the input of a task at one WD requires the ﬁnal task outputs from multiple other WDs. Numerical results show that the proposed method can signiﬁcantly outperform the other representative benchmarks and efﬁciently achieve low complexity with respect to the call graph size.},
	language = {en},
	number = {1},
	urldate = {2025-01-06},
	journal = {IEEE Transactions on Wireless Communications},
	author = {Yan, Jia and Bi, Suzhi and Zhang, Ying Jun and Tao, Meixia},
	month = jan,
	year = {2020},
	pages = {235--250},
	file = {PDF:/home/arthur/Zotero/storage/W6EFE4GC/Yan et al. - 2020 - Optimal Task Offloading and Resource Allocation in Mobile-Edge Computing With Inter-User Task Depend.pdf:application/pdf},
}

@article{jazayeri_autonomous_2021,
	title = {Autonomous computation offloading and auto-scaling the in the mobile fog computing: a deep reinforcement learning-based approach},
	volume = {12},
	issn = {1868-5137, 1868-5145},
	shorttitle = {Autonomous computation offloading and auto-scaling the in the mobile fog computing},
	url = {https://link.springer.com/10.1007/s12652-020-02561-3},
	doi = {10.1007/s12652-020-02561-3},
	abstract = {The Fog Computing (FC) paradigm is rapidly becoming an appropriate framework for the infrastructure related to the Internet of Things (IoT). FC can be a good framework for mobile applications in the IoT. This architecture is referred to as the Mobile Fog Computing (MFC). Modules in the applications can be sent to the Fog or Cloud layer in the event of the lack of resources or increased runtime on the mobile. This increases the efficiency of the whole system. As data is entered sequentially, and the input is given to the modules, the number of executable modules increases. So, this research was conducted to find the best place in order to run the modules that can be on the mobile, Fog, or Cloud. According to the proposed method, first, the Fog Devices (FDs) were locally evaluated using a greedy technique; namely, the sibling nodes followed by the parent and in the second step, a Deep Reinforcement Learning (DRL) algorithm found the best destination to execute the module so as to create a compromise between the power consumption and execution time of the modules. The evaluation results obtained regarding the parameters of the power consumption, execution cost, delay, and network resource usage showed that the proposed method on average is better than the local execution, First-Fit (FF), and standard DRL by 18, 6, and 2\%, respectively.},
	language = {en},
	number = {8},
	urldate = {2025-01-06},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Jazayeri, Fatemeh and Shahidinejad, Ali and Ghobaei-Arani, Mostafa},
	month = aug,
	year = {2021},
	pages = {8265--8284},
	file = {PDF:/home/arthur/Zotero/storage/9XEPLSMU/Jazayeri et al. - 2021 - Autonomous computation offloading and auto-scaling the in the mobile fog computing a deep reinforce.pdf:application/pdf},
}

@ar{bernard_dnpga_2024,
	title = {D-{NPGA} : a new approach for tasks offloading in fog/cloud environment},
	shorttitle = {D-{NPGA}},
	url = {https://ieeexplore.ieee.org/abstract/document/10708605/authors},
	doi = {10.1109/CoDIT62066.2024.10708605},
	abstract = {In recent years, the Internet of Things (IoT) has gained significant attention as a platform for various smart applications that rely on sensors to collect and process data. However, the transmission of data from IoT devices to Cloud data centers can be delayed due to the physical distance between them. Fog computing has emerged as a promising solution to address this issue by bringing the computing resources closer to the IoT devices. One of the challenging problems in Cloud/Fog environment is tasks offloading. A lot of computer work is required to optimize even a small number of tasks. In this paper, we propose a new algorithm based on the Niched Pareto Genetic Algorithm (NPGA) called D-NPGA (Drafting NPGA) to tackle this problem. Our results show that this method can outperform state of the art solution and optimize the total cost and the makespan by 15\%. This algorithm also scales better than its counterparts gaining an important advantage as the number of tasks rises. Besides, it proposes a set of optimal solutions instead of a single one.},
	urldate = {2025-01-06},
	booktitle = {2024 10th {International} {Conference} on {Control}, {Decision} and {Information} {Technologies} ({CoDIT})},
	author = {Bernard, Léo and Yassa, Sonia and Alouache, Lylia},
	month = jul,
	year = {2024},
	note = {ISSN: 2576-3555},
	keywords = {Artificial intelligence, Cloud, Cloud computing, Costs, Edge computing, Fog, Information technology, Intelligent sensors, Internet of Things, NPGA, Pareto optimization, Tasks offloading, Testing, Wireless communication},
	pages = {193--198},
	file = {IEEE Xplore Abstract Record:/home/arthur/Zotero/storage/E54ILJY9/authors.html:text/html},
}

@misc{guo_algorithmics_2024,
	title = {Algorithmics and {Complexity} of {Cost}-{Driven} {Task} {Offloading} with {Submodular} {Optimization} in {Edge}-{Cloud} {Environments}},
	url = {http://arxiv.org/abs/2411.15687},
	doi = {10.48550/arXiv.2411.15687},
	abstract = {Emerging applications such as autonomous driving pose the challenge of efficient cost-driven offloading in edge-cloud environments. This involves assigning tasks to edge and cloud servers for separate execution, with the goal of minimizing the total service cost including communication and computation costs. In this paper, observing that the intra-cloud communication costs are relatively low and can often be neglected in many real-world applications, we consequently introduce the so-called communication assumption which posits that the intra-cloud communication costs are not higher than the inter-partition communication cost between cloud and edge servers, nor the cost among edge servers. As a preliminary analysis, we first prove that the offloading problem without the communication assumption is NP-hard, using a reduction from MAX-CUT. Then, we show that the offloading problem can be modeled as a submodular minimization problem, making it polynomially solvable. Moreover, this polynomial solvability remains even when additional constraints are imposed, such as when certain tasks must be executed on edge servers due to latency constraints. By combining both algorithmics and computational complexity results, we demonstrate that the difficulty of the offloading problem largely depends on whether the communication assumption is satisfied. Lastly, extensive experiments are conducted to evaluate the practical performance of the proposed algorithm, demonstrating its significant advantages over the state-of-the-art methods in terms of efficiency and cost-effectiveness.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Guo, Longkun and Lin, Jiawei and Xu, Xuanming and Li, Peng},
	month = nov,
	year = {2024},
	note = {arXiv:2411.15687 [cs]
version: 1},
	keywords = {Computer Science - Computational Complexity, Computer Science - Discrete Mathematics, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/home/arthur/Zotero/storage/CEBB8E7Y/Guo et al. - 2024 - Algorithmics and Complexity of Cost-Driven Task Offloading with Submodular Optimization in Edge-Clou.pdf:application/pdf;Snapshot:/home/arthur/Zotero/storage/Z45ANHVE/2411.html:text/html},
}

@article{jin_task_2024,
	title = {Task offloading for multi-server edge computing in industrial {Internet} with joint load balance and fuzzy security},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-79464-2},
	doi = {10.1038/s41598-024-79464-2},
	abstract = {The industrial Internet revolutionizes traditional manufacturing through the incorporation of technologies such as real-time production optimization, big data analysis, etc. Computing resource-constrained industrial terminals struggle to effectively execute latency-sensitive and computation-intensive tasks triggered by these technologies. Edge computing (EC) emerges as a promising paradigm for offloading tasks from terminals to the adjacent edge servers, offering the potentiality to augment the computational capacities for industrial terminals. However, the development of accurate offloading strategies poses a prominent challenge for EC in the industrial Internet. Incorrect offloading strategies will misguide the task offloading procedure, resulting in adverse consequences. In this paper, we study the latency-aware multi-server partial EC task offloading problem in the industrial Internet with the consideration of joining load balancing and security protection to provide accurate strategies. Firstly, we establish a task offloading model that supports partial offloading, facilitating latency reduction, task offloading across multiple edge servers with load balance, and accommodation of fuzzy task risks. We quantify the established model as a constrained optimization formulation and prove its NP-hardness. Secondly, to solve the composite offloading strategy comprising both the offloading location and offloading ratio derived from our model, we propose a bi-layer offloading algorithm with joint load balance and fuzzy security, which is based on the adaptive genetic algorithm and simulated annealing particle swarm optimization. Based on extensive experimental results, we find that the established model is effective in reducing the objective value, with a respective decrease of 27\% and 46\% compared to full execution in edge servers and local execution in industrial terminals. Furthermore, the proposed offloading algorithm exhibits superior performance in terms of solution accuracy compared to existing algorithms.},
	language = {en},
	number = {1},
	urldate = {2025-01-06},
	journal = {Scientific Reports},
	author = {Jin, Xiaomin and Zhang, Shuai and Ding, Yurong and Wang, Zhongmin},
	month = nov,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Information technology},
	pages = {27813},
	file = {Full Text PDF:/home/arthur/Zotero/storage/P4JHFVID/Jin et al. - 2024 - Task offloading for multi-server edge computing in industrial Internet with joint load balance and f.pdf:application/pdf},
}

@misc{zhang_survey_2024,
	title = {A {Survey} of {Computation} {Offloading} with {Task} {Type}},
	url = {http://arxiv.org/abs/2401.01017},
	doi = {10.48550/arXiv.2401.01017},
	abstract = {Computation task offloading plays a crucial role in facilitating computation-intensive applications and edge intelligence, particularly in response to the explosive growth of massive data generation. Various enabling techniques, wireless technologies and mechanisms have already been proposed for task offloading, primarily aimed at improving the quality of services (QoS) for users. While there exists an extensive body of literature on this topic, exploring computation offloading from the standpoint of task types has been relatively underrepresented. This motivates our survey, which seeks to classify the state-of-the-art (SoTA) from the task type point-of-view. To achieve this, a thorough literature review is conducted to reveal the SoTA from various aspects, including architecture, objective, offloading strategy, and task types, with the consideration of task generation. It has been observed that task types are associated with data and have an impact on the offloading process, including elements like resource allocation and task assignment. Building upon this insight, computation offloading is categorized into two groups based on task types: static task-based offloading and dynamic task-based offloading. Finally, a prospective view of the challenges and opportunities in the field of future computation offloading is presented.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Zhang, Siqi and Yi, Na and Ma, Yi},
	month = jun,
	year = {2024},
	note = {arXiv:2401.01017 [cs]
version: 3},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/home/arthur/Zotero/storage/ES9EH73B/Zhang et al. - 2024 - A Survey of Computation Offloading with Task Type.pdf:application/pdf;Snapshot:/home/arthur/Zotero/storage/8U5UZBWW/2401.html:text/html},
}

@article{sarkar_deep_2022,
	title = {Deep learning-based energy-efficient computational offloading strategy in heterogeneous fog computing networks},
	volume = {78},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-022-04461-z},
	doi = {10.1007/s11227-022-04461-z},
	abstract = {In the era of the Internet of Things (IoT), the volume of data is increasing immensely causing rapid growth in network data communication and data congestion. Computational offloading thus becomes a crucial and imperative action in terms of delay-sensitive task completion and data processing for the resource constraint end-users. Nowadays fog computing, as a complement of cloud computing, has emerged a well-known concept in terms of enhancing data processing capability as well as energy conservation in low-powered networks. In this paper, we consider a heterogeneous fog-cloud network architecture where the data processing is performed on the local or remote computing device by adopting a binary offloading policy. Based on the proposed system model, we calculate the total delay and energy consumption of data processing throughout the network and formulate a mixed-integer optimization problem to jointly optimize the offloading decision and bandwidth allocation. In order to solve such an NP-hard problem, we have proposed a deep-learning-based binary offloading strategy that employs multiple parallel deep neural networks (DNNs) to make offloading decisions. Such offloading decisions are subsequently placed in a relay memory system to train and test all DNNs. Simulation results show a near-optimal performance of the proposed offloading strategy while remarkably maintaining the quality of service by decreasing overall delay and energy consumption.},
	language = {en},
	number = {13},
	urldate = {2025-01-06},
	journal = {The Journal of Supercomputing},
	author = {Sarkar, Indranil and Kumar, Sanjay},
	month = sep,
	year = {2022},
	keywords = {Artificial Intelligence, Computational offloading, Deep neural networks, Energy consumption, Fog computing, Quantum Computing, Resource allocation},
	pages = {15089--15106},
}

@inproceedings{deng_fogbus2_2021,
	address = {Virtual Event China},
	title = {{FogBus2}: a lightweight and distributed container-based framework for integration of {IoT}-enabled systems with edge and cloud computing},
	isbn = {978-1-4503-8465-0},
	shorttitle = {{FogBus2}},
	url = {https://dl.acm.org/doi/10.1145/3460866.3461768},
	doi = {10.1145/3460866.3461768},
	abstract = {Edge/Fog computing is a novel computing paradigm that provides resource-limited Internet of Things (IoT) devices with scalable computing and storage resources. Compared to cloud computing, edge/fog servers have fewer resources, but they can be accessed with higher bandwidth and less communication latency. Thus, integrating edge/fog and cloud infrastructures can support the execution of diverse latency-sensitive and computation-intensive IoT applications. Although some frameworks attempt to provide such integration, there are still several challenges to be addressed, such as dynamic scheduling of different IoT applications, scalability mechanisms, multi-platform support, and supporting different interaction models. To overcome these challenges, we propose a lightweight and distributed container-based framework, called FogBus2. It provides a mechanism for scheduling heterogeneous IoT applications and implements several scheduling policies. Also, it proposes an optimized genetic algorithm to obtain fast convergence to wellsuited solutions. Besides, it offers a scalability mechanism to ensure efficient responsiveness when either the number of IoT devices increases or the resources become overburdened. Also, the dynamic resource discovery mechanism of FogBus2 assists new entities to quickly join the system. We have also developed two IoT applications, called Conway’s Game of Life and Video Optical Character Recognition to demonstrate the effectiveness of FogBus2 for handling real-time and non-real-time IoT applications. Experimental results show FogBus2’s scheduling policy improves the response time of IoT applications by 53\% compared to other policies. Also, the scalability mechanism can reduce up to 48\% of the queuing waiting time compared to frameworks that do not support scalability.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {International} {Workshop} on {Big} {Data} in {Emergent} {Distributed} {Environments}},
	publisher = {ACM},
	author = {Deng, Qifan and Goudarzi, Mohammad and Buyya, Rajkumar},
	month = jun,
	year = {2021},
	pages = {1--8},
	file = {PDF:/home/arthur/Zotero/storage/WZEEWFJ5/Deng et al. - 2021 - FogBus2 a lightweight and distributed container-based framework for integration of IoT-enabled syst.pdf:application/pdf},
}

@article{zhang2022osttd,
  title={OSTTD: Offloading of Splittable Tasks with Topological Dependence in Multi-Tier Computing Networks},
  author={Zhang, Rui and Chu, Xuesen and Ma, Ruhui and Zhang, Meng and Lin, Liwei and Gao, Honghao and Guan, Haibing},
  journal={IEEE Journal on Selected Areas in Communications},
  year={2022},
  publisher={IEEE}
}


@inproceedings{WiesnerThamsen_LEAF_2021,
  author={Wiesner, Philipp and Thamsen, Lauritz},
  booktitle={2021 IEEE 5th International Conference on Fog and Edge Computing (ICFEC)}, 
  title={{LEAF}: Simulating Large Energy-Aware Fog Computing Environments}, 
  year={2021},
  pages={29-36},
  doi={10.1109/ICFEC51620.2021.00012}
}
@article{ismail_computing_2021,
	title = {Computing {Server} {Power} {Modeling} in a {Data} {Center}: {Survey}, {Taxonomy}, and {Performance} {Evaluation}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Computing {Server} {Power} {Modeling} in a {Data} {Center}},
	url = {https://dl.acm.org/doi/10.1145/3390605},
	doi = {10.1145/3390605},
	abstract = {Data centers are large-scale, energy-hungry infrastructure serving the increasing computational demands as the world is becoming more connected in smart cities. The emergence of advanced technologies such as cloud-based services, internet of things (IoT), and big data analytics has augmented the growth of global data centers, leading to high energy consumption. This upsurge in energy consumption of the data centers not only incurs the issue of surging high cost (operational and maintenance) but also has an adverse effect on the environment. Dynamic power management in a data center environment requires the cognizance of the correlation between the system and hardware-level performance counters and the power consumption. Power consumption modeling exhibits this correlation and is crucial in designing energy-efficient optimization strategies based on resource utilization. Several works in power modeling are proposed and used in the literature. However, these power models have been evaluated using different benchmarking applications, power-measurement techniques, and error-calculation formulas on different machines. In this work, we present a taxonomy and evaluation of 24 software-based power models using a unified environment, benchmarking applications, power-measurement techniques, and error formulas, with the aim of achieving an objective comparison. We use different server architectures to assess the impact of heterogeneity on the models’ comparison. The performance analysis of these models is elaborated in the article.},
	language = {en},
	number = {3},
	urldate = {2025-03-11},
	journal = {ACM Computing Surveys},
	author = {Ismail, Leila and Materwala, Huned},
	month = may,
	year = {2021},
	pages = {1--34},
	file = {PDF:/home/arthur/Zotero/storage/5VCV28EQ/Ismail and Materwala - 2021 - Computing Server Power Modeling in a Data Center Survey, Taxonomy, and Performance Evaluation.pdf:application/pdf},
}

@misc{googleDataCenters,
  author = {{Google}},
  title = {{Google Data Center Locations}},
  howpublished = {\url{https://www.google.com/about/datacenters/locations/}},
  year = {2024},
  note = {Accessed: 2024-03-23}
}

@article{Deb2002AFA,
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II},
  author={Kalyanmoy Deb and Samir Agrawal and Amrit Pratap and T. Meyarivan},
  journal={IEEE Trans. Evol. Comput.},
  year={2002},
  volume={6},
  pages={182-197},
  url={https://api.semanticscholar.org/CorpusID:9914171}
}

@inproceedings{horn1994npga,
author = {Horn, Jeffrey and Nafpliotis, N. and Goldberg, D.E.},
year = {1994},
month = {07},
pages = {82 - 87 vol.1},
title = {A Niched Pareto Genetic Algorithm for Multi-Objective Optimization},
volume = {1},
isbn = {0-7803-1899-4},
journal = {Proceedings of the 1st IEEE Conference on Computation Evolutionary},
doi = {10.1109/ICEC.1994.350037}
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{such2018deepneuroevolutiongeneticalgorithms,
      title={Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}, 
      author={Felipe Petroski Such and Vashisht Madhavan and Edoardo Conti and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
      year={2018},
      eprint={1712.06567},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1712.06567}, 
}

@misc{hendrycks2023gaussianerrorlinearunits,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.08415}, 
}



@article{rahmani_optimizing_2025,
	title = {Optimizing task offloading with metaheuristic algorithms across cloud, fog, and edge computing networks: {A} comprehensive survey and state-of-the-art schemes},
	volume = {45},
	issn = {22105379},
	shorttitle = {Optimizing task offloading with metaheuristic algorithms across cloud, fog, and edge computing networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210537924001252},
	doi = {10.1016/j.suscom.2024.101080},
	abstract = {The Internet of Things (IoT) significantly impacts various industries, enabling better connectivity and real-time data exchange for applications ranging from smart cities to healthcare. Integrating cloud, fog, and edge computing is essential for managing increased data and processing needs as IoT networks become complex. Cloud computing provides extensive storage and powerful computing capabilities but can experience delays due to the distance data must travel. Fog computing addresses these delays by processing data closer to its source, while edge computing reduces them even further by processing data directly on IoT devices. Effective management of these computing layers requires strategic task offloading, which involves moving tasks to the most appropriate computing layer to balance latency, energy consumption, and operational efficiency. Several strategies have been developed to optimize network communication and task offloading, with metaheuristic algorithms emerging as promising approaches. Inspired by natural processes, these algorithms are skilled at searching complex spaces to find near-optimal solutions for efficient and dynamic task offloading. This review provides a detailed analysis of how metaheuristic algorithms optimize task offloading. It evaluates their effectiveness in improving system performance, managing resources, and reducing costs. The review also identifies the current challenges in this area and suggests future research directions to advance this field.},
	language = {en},
	urldate = {2025-06-10},
	journal = {Sustainable Computing: Informatics and Systems},
	author = {Rahmani, Amir Masoud and Haider, Amir and Khoshvaght, Parisa and Gharehchopogh, Farhad Soleimanian and Moghaddasi, Komeil and Rajabi, Shakiba and Hosseinzadeh, Mehdi},
	month = jan,
	year = {2025},
	pages = {101080},}

@article{pakmehr_etfc_2024,
	title = {{ETFC}: {Energy}-efficient and deadline-aware task scheduling in fog computing},
	volume = {43},
	issn = {22105379},
	shorttitle = {{ETFC}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210537924000337},
	doi = {10.1016/j.suscom.2024.100988},
	abstract = {The Internet of Things (IoT) is constantly evolving and expanding. However, due to the limited IoT resources, it is intertwined with fog computing to use their resources to compensate for the limitations of IoT resources. On the other hand, fog devices face challenges, such as resource heterogeneity, high distribution, dynamism, and limitations, so an efficient task scheduling approach is needed to deploy fog computing resources effectively and improve the quality of service (QoS). This work mathematically formulates the task scheduling problem to minimize energy consumption and cost and improve QoS by reducing response time and deadline violation times of IoT tasks. Then, it proposes an Energy-efficient and deadline-Aware Task scheduling in Fog Computing (ETFC) method that predicts the traffic of fog nodes by a Support Vector Machine (SVM) and divides them into lowtraffic and high-traffic groups. Next, the ETFC method schedules the low-traffic part with an algorithm based on reinforcement learning using the proposed ICLA-SOA, which is an algorithm based on irregular cellular learning automata and schedules the tasks of the high-traffic part with a metaheuristic algorithm using the proposed Non-dominated Sorting Genetic Algorithm (NSGA-III). The simulation results demonstrate that the ETFC method exhibits up to an 84 \% enhancement in response time, up to a 33 \% reduction in energy consumption, up to a 30 \% decrease in costs, and up to a 28 \% advancement in meeting task deadlines compared to other methods.},
	language = {en},
	urldate = {2025-06-10},
	journal = {Sustainable Computing: Informatics and Systems},
	author = {Pakmehr, Amir and Gholipour, Majid and Zeinali, Esmaeil},
	month = sep,
	year = {2024},
	pages = {100988},
	file = {PDF:/home/arthur/Zotero/storage/6CIZLKWT/Pakmehr et al. - 2024 - ETFC Energy-efficient and deadline-aware task scheduling in fog computing.pdf:application/pdf},
}